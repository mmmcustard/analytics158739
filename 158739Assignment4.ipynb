{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6046cd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a0db5ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Union\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed603f20",
   "metadata": {},
   "source": [
    "# 158739 Assignment 4 - Does having wealthy parents make you a better tennis player?\n",
    "\n",
    "#### Student name: James Evans and Hayley Wikeepa\n",
    "#### Student ID: 12211872\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "\n",
    "\n",
    "Does having wealth parent make you a better tennis player? There are lots of ways to analise this. We can look at if the current world top tennis players are mostly from wealth families. We can look if being from a wealthy family makes it more likely you will win when playing profesional tennis. Assuming this is true we can look if having wealthy parents is \"priced in\" to the betting odds when betting on tennis matches.\n",
    "\n",
    "Two things that make us wonder about tennis and wealthy parents. Firstly there are two of the top 100 woman tennis players have billionare parent. Second a tweet one of us saw wondering why so many formula one drivers fathers have hyperlinks in wikipedia (hinting that you need a rich father to become a formular 1 driver).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Datasets used:\n",
    "- ATP (Mens profesional tennis assossiation) and WTA (Womens profesional tennis assossiation) tennis player lists\n",
    "- Tennis results from the past ?? years\n",
    "- Tennis betting odds from the last ?? years\n",
    "- List of tennis players that have wiki pages, and a boolean if there parents have wiki pages\n",
    "- chat gpt answers to the question \"did this tennis player have wealthy parents?\"\n",
    "\n",
    "\n",
    "#### Dataset sources: \n",
    "\n",
    "- \n",
    "- \n",
    "\n",
    "\n",
    "### Research Questions\n",
    "\n",
    "1. Is having wealth parents a predictor of becoming a profectional tennis player?\n",
    "2. Is having wealth parents a predictor of future sucssess as a profectional tennis player?\n",
    "3. Is there money to be made by taking into account if a tennis player has wealth parents, when betting on a tennis mathches?\n",
    "\n",
    "\n",
    "### Executive Summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cee07d",
   "metadata": {},
   "source": [
    "## What do we need to achive?\n",
    "\n",
    "\n",
    "### Ideas of what we could find out\n",
    "\n",
    "First we could see if there is a disporportinate amount of pro tennis players that have wealth parents (I am sure there will be).\n",
    "\n",
    "Would could see if there is a correlation between rich partents and tennis success when playing as a pro (look at results from pro matches). \n",
    "we could look at win % vs wealthy parents\n",
    "\n",
    "If answer is yes, wealth parents make you play better as a pro, then we can look to see if this is taken into account in the betting odds.\n",
    "For each player, we could work out how much you would have won / lost, if you had bet a constant amount on that player, over there last x number of games. We can then have a way of saying who is a profitable player and who is an unprofitable player to bet on. We can then see if this correlates with if they have rich/famous parents.\n",
    "\n",
    "\n",
    "### Marking info\n",
    "\n",
    "- Data Acquisition -     20\n",
    "    * Diversity of sources (at least one must be dynamic – full marks for using both APIs and web scraping – penalties will be applied for re-using examples from class)\n",
    "    * Appropriate use of merging and concatenation.\n",
    "    \n",
    "- Data Wrangling and EDA  -  30\n",
    "- Data Analysis -  35\n",
    "- Originality and challenge 15\n",
    "\n",
    "BONUS\n",
    "- Big Data ProcessingTechniques - 5\n",
    "\n",
    "### Other notes from the brief\n",
    "Clearly introduce your problem domain, articulate your research questions and provide an executive summary at the beginning. Follow the provided Jupyter notebook template.\n",
    "You must document and explain the reasoning behind the coding steps you are taking and provide explanations of all your graphs and tables as is appropriate. Make sure you label all aspects of your graphs.\n",
    "The activities listed under the five stages in the workflow diagram above are a guide only. This means that operations like group-by statements as well as pivot tables could be a part of the ‘Data Wrangling’ phase as EDA, and not only a part of the data analysis phase. Finally, please run your report through an external spell checker and feel free to use ChatGPT judiciously to help you as discussed in class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cd940c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Set up the API endpoint and your API key\n",
    "api_key = '376e391a55b777dce519f686833a8830'\n",
    "sport = 'tennis_atp_us_open'\n",
    "region = 'us'  # Empty string for worldwide\n",
    "\n",
    "# Define other parameters for the API request\n",
    "params = {\n",
    "    'sport': sport,\n",
    "    'region': 'us',\n",
    "    'mkt': 'h2h',\n",
    "    'oddsFormat': 'decimal',\n",
    "    'dateFormat': 'iso'\n",
    "}\n",
    "\n",
    "# Make the API request\n",
    "url = f'https://api.the-odds-api.com/v4/sports/{sport}/odds'\n",
    "response = requests.get(url, params={'apiKey': api_key, **params})\n",
    "\n",
    "# Process the response data\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Process and analyze the odds data as per your needs\n",
    "else:\n",
    "    print(f\"Error: {response.status_code} - {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a766b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tornaments[tornaments[\"city\"]==\"Brisbane\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05cc010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise https://rapidapi.com/sportcontentapi/api/tennis-live-data API\n",
    "# ATP - mens\n",
    "# WTA - womens\n",
    "\n",
    "api_key = '8556d4b2f5mshddae5c2b7778158p1b7b83jsn131f0acf695b'\n",
    "tennis_live_headers = {\n",
    "    \"x-rapidapi-key\": api_key,\n",
    "    \"x-rapidapi-host\": \"tennis-live-data.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "ultimate_tennis_headers =  {\n",
    "'X-RapidAPI-Key': api_key,\n",
    "'X-RapidAPI-Host': 'ultimate-tennis1.p.rapidapi.com'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c3d1911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WTA tornaments\n",
    "# Run sparingly as we only have 250 API calls to rapidapi.com every month.\n",
    "\n",
    "wta_base_url = \"https://tennis-live-data.p.rapidapi.com/tournaments/WTA/\"\n",
    "\n",
    "wta_tornaments_dfs = []\n",
    "\n",
    "for year in range(datetime.datetime.now().year - 4, datetime.datetime.now().year):\n",
    "    endpoint = wta_base_url + str(year)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        df = pd.DataFrame(data['results'])\n",
    "        df.set_index('id', inplace=True)\n",
    "        wta_tornaments_dfs.append(df)\n",
    "    else:\n",
    "        print(\"Error occurred for year\", year, \":\", response.status_code)\n",
    "\n",
    "wta_tornaments.index.name = 'id'\n",
    "\n",
    "wta_tornaments.to_csv('wta_tornaments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789aadd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(wta_tornaments)",
    "url = \"https://tennis-live-data.p.rapidapi.com/matches-results/1050\"\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07a420e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = response.json()\n",
    "data['results']['matches']\n",
    "df = pd.DataFrame(data['results']['matches'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c3d1911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WTA tornaments\n",
    "# Run sparingly as we only have 250 API calls to rapidapi.com every month.\n",
    "\n",
    "wta_base_url = \"https://tennis-live-data.p.rapidapi.com/tournaments/WTA/\"\n",
    "\n",
    "wta_tornaments_dfs = []\n",
    "\n",
    "for year in range(datetime.datetime.now().year - 4, datetime.datetime.now().year):\n",
    "    endpoint = wta_base_url + str(year)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        df = pd.DataFrame(data['results'])\n",
    "        df.set_index('id', inplace=True)\n",
    "        wta_tornaments_dfs.append(df)\n",
    "    else:\n",
    "        print(\"Error occurred for year\", year, \":\", response.status_code)\n",
    "\n",
    "wta_tornaments = pd.concat(wta_tornaments_dfs)\n",
    "wta_tornaments.to_csv('wta_tornaments.csv')\n",
    "# print(wta_tornaments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "07a420e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# add all tornaments into one dataframe\n",
    "\n",
    "tornaments = pd.concat([atp_tornaments, wta_tornaments])\n",
    "tornaments.to_csv('tornaments.csv')\n",
    "tornaments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7b54a8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_base_url = \"https://ultimate-tennis1.p.rapidapi.com/tournament_results/\"\n",
    "results_dfs = []\n",
    "for index, row in tornaments.iterrows():\n",
    "    id_value = row['id']\n",
    "    year_value = row['season']\n",
    "    code = \"\"\n",
    "    if row['code'] == \"ATP\":\n",
    "    else if\n",
    "        code = \"/wta\"\n",
    "    \n",
    "    \n",
    "    endpoint = results_base_url+code+str(id_value)+\"/\"+str(year_value)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        data = result_response.json()\n",
    "        df['Tournament_ID'] = data['Tournament ID']\n",
    "        df['Location'] = data['Location']\n",
    "        df['Year'] = data['Year']\n",
    "        results_dfs.append(df)\n",
    "    else:\n",
    "    print(\"Error occurred for id \", id_value, \" year\", year_value, \":\", result_response.status_code)\n",
    "   \n",
    "results_df = pd.concat(results_dfs)\n",
    "results_df.to_csv('results_df.csv')    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cd0268a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_base_url = \"https://ultimate-tennis1.p.rapidapi.com/tournament_results/\"\n",
    "results_dfs = []\n",
    "\n",
    "id_value = \"416\"\n",
    "year_value = \"2022\"\n",
    "endpoint = results_base_url+str(id_value)+\"/\"+str(year_value)\n",
    "result_response = requests.get(endpoint, headers=headers)\n",
    "if result_response.status_code == 200:\n",
    "    data = result_response.json()\n",
    "    df['Tournament_ID'] = data['Tournament ID']\n",
    "    df['Location'] = data['Location']\n",
    "    df['Year'] = data['Year']\n",
    "else:\n",
    "    print(\"Error occurred for id \", id_value, \" year\", year_value, \":\", result_response.status_code)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0f52a3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://ultimate-tennis1.p.rapidapi.com/tournament_results/416/2022\"\n",
    "\n",
    "headers = {\n",
    "\t\"X-RapidAPI-Key\": \"8556d4b2f5mshddae5c2b7778158p1b7b83jsn131f0acf695b\",\n",
    "\t\"X-RapidAPI-Host\": \"ultimate-tennis1.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "053f3021",
   "metadata": {},
   "outputs": [],
   "source": [
    "        data = response.json()\n",
    "        df = pd.DataFrame(data['data'])\n",
    "        df['Tournament_ID'] = data['Tournament ID']\n",
    "        df['Location'] = data['Location']\n",
    "        df['Year'] = data['Year']\n",
    "        df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6110c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "# Run sparingly as we only have 250 API calls to rapidapi.com every month.\n",
    "\n",
    "results_url = \"https://tennis-live-data.p.rapidapi.com/tournaments/WTA/\"\n",
    "\n",
    "wta_tornaments_dfs = []\n",
    "\n",
    "for year in range(datetime.datetime.now().year - 4, datetime.datetime.now().year):\n",
    "    endpoint = wta_base_url + str(year)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        df = pd.DataFrame(data['results'])\n",
    "        df.set_index('id', inplace=True)\n",
    "        wta_tornaments_dfs.append(df)\n",
    "    else:\n",
    "        print(\"Error occurred for year\", year, \":\", response.status_code)\n",
    "\n",
    "wta_tornaments = pd.concat(wta_tornaments_dfs)\n",
    "wta_tornaments.to_csv('wta_tornaments.csv')\n",
    "# print(wta_tornaments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3aa9f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test of ultimate_tennis_headers torn results\n",
    "url = \"https://ultimate-tennis1.p.rapidapi.com/tournament_list/atp/2015/atpgs\"\n",
    "response = requests.get(url, headers=ultimate_tennis_headers)\n",
    "data = response.json()\n",
    "df = pd.DataFrame(data['Tournaments'])\n",
    "df.set_index('ID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9d9569a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add all tornaments into one dataframe\n",
    "\n",
    "tornaments = pd.concat([atp_tornaments, wta_tornaments])\n",
    "tornaments.index.name = 'id'\n",
    "tornaments.to_csv('tornaments.csv')\n",
    "tornaments.head()",
    "data = response.json()\n",
    "df = pd.DataFrame(data['Tournaments'])\n",
    "df.set_index('ID', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14423bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATP players\n",
    "\n",
    "atp_players_url = \"https://tennis-live-data.p.rapidapi.com/players/ATP\"\n",
    "atp_players_response = requests.get(atp_players_url, headers=tennis_live_headers)\n",
    "\n",
    "atp_json_data = atp_players_response.json()\n",
    "atp_data = atp_json_data['results']['players']\n",
    "atp_players = pd.json_normalize(atp_data)\n",
    "atp_players.set_index('id', inplace=True)\n",
    "atp_players.to_csv('atp_players.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a136a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WTA players\n",
    "\n",
    "wta_players_url = \"https://tennis-live-data.p.rapidapi.com/players/WTA\"\n",
    "WTA_players_response = requests.get(wta_players_url, headers=tennis_live_headers)\n",
    "\n",
    "WTA_json_data = WTA_players_response.json()\n",
    "WTA_data = WTA_json_data['results']['players']\n",
    "wta_players = pd.json_normalize(WTA_data)\n",
    "wta_players.set_index('id', inplace=True)\n",
    "wta_players.to_csv('wta_players.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bcfb4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "#<<<<<<< Updated upstream\n",
     "#all players\n",
    "\n",
    "players_df = pd.concat([atp_players, df_temp], ignore_index=True)\n",
    "players_df.set_index('id', inplace=True)\n",
    "\n",
    "players_df.to_csv('players.csv')\n",
    "# players_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7b64d826",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get all ATP results from 2019 - 2022 for all tournaments into one dataframe\n",
    "\n",
    "# Run sparingly as we only have 250 API calls to rapidapi.com every month.\n",
    "\n",
    "base_url = \"https://ultimate-tennis1.p.rapidapi.com/tournament_list/atp/{year}/atpgs\"\n",
    "\n",
    "# get all ATP tounament results\n",
    "ATP_tornaments_list = []\n",
    "\n",
    "headers = {\n",
    "\t\"X-RapidAPI-Key\": \"8556d4b2f5mshddae5c2b7778158p1b7b83jsn131f0acf695b\",\n",
    "    \"X-RapidAPI-Host\": \"ultimate-tennis1.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "for year in range(datetime.datetime.now().year - 4, datetime.datetime.now().year):\n",
    "    url = base_url.format(year=year)\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        df = pd.DataFrame(data['Tournaments'])\n",
    "        ATP_tornaments_list.append(df)\n",
    "    else:\n",
    "        print(f\"Error retrieving data for year {year}. Status code: {response.status_code}\")\n",
    "        \n",
    "# combine all dfs in ATP_tournaments list        \n",
    "ATP_tornaments = pd.concat(ATP_tornaments_list)\n",
    "ATP_tornaments = ATP_tornaments.reset_index(drop=True)       \n",
    "\n",
    "# normalise winners column\n",
    "winners_df = pd.json_normalize(ATP_tornaments['Winners'])\n",
    "temp_ATP_df = pd.concat([ATP_tornaments, winners_df], axis=1)\n",
    "ATP_tornaments = temp_ATP_df.drop('Winners', axis=1)\n",
    "\n",
    "# normalise doubles column\n",
    "doubles_df = ATP_tornaments['doubles'].str.split(' / ', expand=True)\n",
    "doubles_df.columns = ['doubles_1', 'doubles_2']\n",
    "temp_df = pd.concat([ATP_tornaments, doubles_df], axis=1)\n",
    "ATP_tornaments = temp_df.drop('doubles', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bcfb4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all players\n",
    "players_df = pd.concat([atp_players, wta_players])\n",
    "players_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a17620a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function returns true if a tennis player had wealth parents\n",
    "#function created by Chat GPT and modified by us.\n",
    "\n",
    "#Note/ function cost about $0.0002 each time it is run. \n",
    "\n",
    "\n",
    "\n",
    "API_ENDPOINT = \"https://api.openai.com/v1/chat/completions\"\n",
    "API_KEY = \"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\"\n",
    "MODEL_NAME = \"gpt-3.5-turbo\"\n",
    "\n",
    "def determine_wealthy_parents(tennis_player_name, country):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\"\n",
    "    }\n",
    "    data = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant who only ever responds with yes or no answers. If you do not have enough information, answer no. Wealthy means the parents were probably in the top 1% of people in there country.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Did the tennis player {tennis_player_name}, from {country}, have wealthy parents?\"}\n",
    "        ],\n",
    "        \"model\": MODEL_NAME\n",
    "    }\n",
    "    response = requests.post(API_ENDPOINT, headers=headers, json=data)\n",
    "    response_json = response.json()\n",
    "    print (response_json)\n",
    "    try:\n",
    "        chat_reply = response_json['choices'][0]['message']['content']\n",
    "        # You can modify the condition below based on the expected response from the model\n",
    "        if \"yes\" in chat_reply.lower():\n",
    "            return True\n",
    "        elif \"no\" in chat_reply.lower():\n",
    "            return False\n",
    "        else:\n",
    "            return None  # Unable to determine the response\n",
    "    except KeyError:\n",
    "        return None  # Invalid response format\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11f5e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "player_name = \"Jeļena Ostapenko\"\n",
    "country = 'Latvia'\n",
    "\n",
    "\n",
    "wealthy_parents = determine_wealthy_parents(player_name, country)\n",
    "\n",
    "\n",
    "if wealthy_parents is not None:\n",
    "    if wealthy_parents:\n",
    "        print(f\"{player_name} had wealthy parents.\")\n",
    "    else:\n",
    "        print(f\"{player_name} did not have wealthy parents.\")\n",
    "else:\n",
    "    print(\"Unable to determine if the player had wealthy parents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6d47fd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a wealthy_parent column\n",
    "\n",
    "players_df[\"wealthy_parents\"] = None\n",
    "players_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8e64e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = players_df[:][0:3]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9cd496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in test.iterrows():\n",
    "    player_name = row['full_name']\n",
    "    country = row['country']\n",
    "    wealthy_parents = determine_wealthy_parents(player_name, country)\n",
    "    test.at[index, 'wealthy_parents'] = wealthy_parents\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ebb7f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a parent_on_wiki column\n",
    "\n",
    "players_df[\"parent_on_wiki\"] = None\n",
    "players_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf65dd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping wiki to find parents. We can use this to check out results from Chat GPT\n",
    "\n",
    "#Todo// Get this working. Currently isssue with find_next_sibling method not returning the info we want.\n",
    "\n",
    "\n",
    "\n",
    "def parents_have_wiki(url):\n",
    "    parents = []  # initialize the parents list\n",
    "    last_name = None  # initialize the last name variable\n",
    "    \n",
    "    # Make a request to the URL and get the HTML response\n",
    "    response = requests.get(url)\n",
    "    html = response.content\n",
    "    \n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Find the \"Personal life\" or \"Early life\" or \"Personal info\" section on the page\n",
    "    personal_life_section = soup.find('span', {'id': 'Early_life'})\n",
    "    if personal_life_section is None:\n",
    "        personal_life_section = soup.find('span', {'id': 'Personal_life'})\n",
    "    if personal_life_section is None:\n",
    "        personal_life_section = soup.find('span', {'id': 'Personal_info'})\n",
    "       \n",
    "        \n",
    "    \n",
    "    # Look for parents in the infobox\n",
    "    infobox = soup.find('table', {'class': 'infobox'})\n",
    "    if infobox:\n",
    "        for row in infobox.find_all('tr'):\n",
    "            th = row.find('th')\n",
    "            if th and 'Parent' in th.text:\n",
    "                td = row.find('td')\n",
    "                for link in td.find_all('a'):\n",
    "                    href = link.get('href')\n",
    "                    if href and '/wiki/' in href and 'Wikipedia:' not in href:\n",
    "                        # Add the parent's URL to the list\n",
    "                        parents.append(href)\n",
    "                        \n",
    "                        \n",
    "    \n",
    "    # Look for parents in the \"Personal life\" section\n",
    "    if personal_life_section is not None:\n",
    "        personal_life_content = personal_life_section #.parent.find_next_sibling('div', {'class': 'hatnote'})\n",
    "        #print(\"persona life content\" + personal_life_content.text)\n",
    "        if personal_life_content is not None:\n",
    "            for link in personal_life_content.find_next('p').find_all('a'):\n",
    "                href = link.get('href')\n",
    "                #print(href)\n",
    "                if href and '/wiki/' in href and 'Wikipedia:' not in href:\n",
    "                    # Check if the hyperlink points to a person with the same last name\n",
    "                    link_text = link.text\n",
    "                    #print(link_text)\n",
    "                    if last_name is None:\n",
    "                        # Get the last name of the person whose page we're on\n",
    "                        name = soup.find('h1', {'id': 'firstHeading'}).text\n",
    "                        last_name = name.split()[-1]\n",
    "                        print(last_name)\n",
    "                    if last_name in link_text:\n",
    "                        # Add the parent's URL to the list\n",
    "                        parents.append(href)\n",
    "    \n",
    "    # Check if both parents have Wikipedia pages\n",
    "    for parent in parents:\n",
    "        parent_url = 'https://en.wikipedia.org' + parent\n",
    "        parent_response = requests.get(parent_url)\n",
    "        parent_soup = BeautifulSoup(parent_response.content, 'html.parser')\n",
    "        parent_title = parent_soup.find('h1', {'id': 'firstHeading'}).text\n",
    "        if parent_title != 'Wikipedia':\n",
    "            # Check if the parent's page exists\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf65dd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape wiki for parents hyperlinks\n",
    "#this takes a longtime.\n",
    "\n",
    "def check_wikipedia_url(url):\n",
    "    response = requests.head(url)\n",
    "    if response.status_code == 200:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "for index, row in players_df.iterrows():\n",
    "    player_name = row['first_name'] + \"_\" + row['last_name']\n",
    "    wiki_url = \"https://en.wikipedia.org/wiki/\" + player_name\n",
    "    \n",
    "    if check_wikipedia_url(wiki_url):\n",
    "        print(wiki_url)\n",
    "        wiki_parents = parents_have_wiki(wiki_url)\n",
    "        print(wiki_url)\n",
    "        print(wiki_parents)\n",
    "        players_df.at[index, 'parent_on_wiki'] = wiki_parents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c25b73d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parents_have_wiki(\"https://en.wikipedia.org/wiki/Jessica_Pegula\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8728dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "parents_have_wiki(\"https://en.wikipedia.org/wiki/Zhang_Zhizhen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "498d6684",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_df.to_csv('players.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fd007f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
